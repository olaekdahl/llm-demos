{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a46b36",
   "metadata": {},
   "source": [
    "# RAG using LangChain\n",
    "\n",
    "This notebook demonstrates a Retrieval-Augmented Generation (RAG) pipeline using LangChain.\n",
    "\n",
    "## Objectives\n",
    "- Load and embed documents\n",
    "- Store in a FAISS vector store using LangChain\n",
    "- Retrieve context from the store\n",
    "- Use a language model to answer questions based on context\n",
    "\n",
    "### Without LangChain\n",
    "- You do everything yourself:\n",
    "    - Manually split documents\n",
    "    - Manually embed them\n",
    "    - Manually build and search the FAISS index\n",
    "    - Manually construct prompts for OpenAI\n",
    "    - Manually handle context formatting and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96b4b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install LangChain and dependencies\n",
    "%pip install langchain langchain-community faiss-cpu openai sentence-transformers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0e73c",
   "metadata": {},
   "source": [
    "## Step 1: Load Documents and Create Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea6fe6",
   "metadata": {},
   "source": [
    "| Scenario                          | Suggested `chunk_size` | `chunk_overlap` |\n",
    "| --------------------------------- | ---------------------- | --------------- |\n",
    "| Short, clean sentences            | 50–100 chars           | 0–10            |\n",
    "| Longer technical paragraphs       | 150–300 chars          | 10–50           |\n",
    "| Preparing for GPT context input   | 500–1000 chars         | 50–200          |\n",
    "| Sentence transformers (embedding) | 100–300 chars          | 20–50           |\n",
    "\n",
    "Generally, you want chunks to:\n",
    "- Be large enough to carry meaning\n",
    "- Be small enough to embed efficiently\n",
    "- Use overlap if your content has context flow (like narrative or technical docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f800f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored documents: 1\n",
      "[Document(metadata={}, page_content='The mitochondria is the powerhouse of the cell.\\nPhotosynthesis occurs in the chloroplasts of plant cells.\\nDNA is stored in the nucleus.\\nProteins are synthesized by ribosomes.\\nATP provides energy for cellular processes.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Sample data\n",
    "text = \"\"\"The mitochondria is the powerhouse of the cell.\n",
    "Photosynthesis occurs in the chloroplasts of plant cells.\n",
    "DNA is stored in the nucleus.\n",
    "Proteins are synthesized by ribosomes.\n",
    "ATP provides energy for cellular processes.\"\"\"\n",
    "\n",
    "# Split into documents\n",
    "# CharacterTextSplitter does not split by character count alone,\n",
    "# it first splits by newlines (\\n) or double newlines into paragraphs or lines, \n",
    "# then applies the chunk_size limit.\n",
    "# Because CharacterTextSplitter assumes paragraph boundaries first, \n",
    "# it tries to chunk within each paragraph, not across them.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0)\n",
    "documents = text_splitter.create_documents([text])\n",
    "\n",
    "# Embedding model - GPU\n",
    "# embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Embedding model - CPU\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"device\": \"cpu\"},\n",
    ")\n",
    "\n",
    "# Create FAISS vector store\n",
    "db = FAISS.from_documents(documents, embedding)\n",
    "print(\"Stored documents:\", len(documents))\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb765e",
   "metadata": {},
   "source": [
    "## Step 2: Create Retriever and Ask Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "199aec0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20363/3383796297.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matching chunks:\n",
      "- The mitochondria is the powerhouse of the cell.\n",
      "Photosynthesis occurs in the chloroplasts of plant cells.\n",
      "DNA is stored in the nucleus.\n",
      "Proteins are synthesized by ribosomes.\n",
      "ATP provides energy for cellular processes.\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "query = \"What produces energy in cells?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(\"Top matching chunks:\")\n",
    "for d in docs:\n",
    "    print(\"-\", d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806801f4",
   "metadata": {},
   "source": [
    "## Step 3: RAG Chain with OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d4e157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20363/3311627955.py:3: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(temperature=0.3)\n",
      "/tmp/ipykernel_20363/3311627955.py:11: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = rag_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "ATP provides energy for cellular processes.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = OpenAI(temperature=0.3)\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "result = rag_chain({\"query\": query})\n",
    "print(\"Answer:\", result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24e0ec",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- LangChain simplified our RAG workflow.\n",
    "- We created a retriever from vector data.\n",
    "- Used LangChain’s `RetrievalQA` to fetch relevant info and answer questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
