{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ab5524",
   "metadata": {},
   "source": [
    "### 1. Recurrent Neural Networks (RNNs)\n",
    "- **Key idea:** Process sequences one step at a time.\n",
    "- **LSTM** (Long Short-Term Memory) and **GRU** (Gated Recurrent Units) were introduced to deal with vanishing gradients and capture longer dependencies.\n",
    "\n",
    "### 2. Sequence-to-Sequence (Seq2Seq) Models\n",
    "- Popularized by **Google Translate (2014)**.\n",
    "- Used an **encoder-decoder architecture** built on RNNs to map input sequences to output sequences (e.g., translations).\n",
    "\n",
    "### 3. Seq2Seq with Attention\n",
    "- **Attention mechanism** added (Bahdanau et al., 2015) to allow the decoder to **dynamically focus** on relevant parts of the input.\n",
    "- Greatly improved performance, especially on longer sequences.\n",
    "\n",
    "### 4. Transformer (2017)\n",
    "- Introduced in “**Attention Is All You Need**.”\n",
    "- Replaced RNNs entirely with **self-attention** and **positional encodings** for parallelization and better long-range modeling.\n",
    "- Became the foundation for models like **BERT**, **GPT**, **T5**, etc.\n",
    "\n",
    "### 5. GPT (2018 onward)\n",
    "- Built on the **Transformer decoder**.\n",
    "- Trained as a **causal language model** (predict the next word).\n",
    "- Successive versions scaled up data, model size, and training time:\n",
    "  - **GPT-2**\n",
    "  - **GPT-3**\n",
    "  - **ChatGPT** (fine-tuned **GPT-3.5/4** with RLHF).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
