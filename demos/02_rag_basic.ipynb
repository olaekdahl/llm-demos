{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258b101f",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) - Basic Demo\n",
    "\n",
    "This notebook demonstrates how RAG works using FAISS and OpenAI's GPT API (or a local model substitute).\n",
    "\n",
    "## Objectives\n",
    "- Ingest documents and chunk them\n",
    "- Create embeddings and store in a vector DB (FAISS)\n",
    "- Retrieve top-k chunks based on query\n",
    "- Send retrieved context to a language model to generate an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "%pip install faiss-cpu sentence-transformers openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57290eb2",
   "metadata": {},
   "source": [
    "## Step 1: Prepare and Embed Document Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Sample document split into chunks\n",
    "doc_chunks = [\n",
    "    \"The mitochondria is the powerhouse of the cell.\",\n",
    "    \"Photosynthesis occurs in the chloroplasts of plant cells.\",\n",
    "    \"DNA contains genetic information and is located in the nucleus.\",\n",
    "    \"Proteins are synthesized by ribosomes using mRNA.\",\n",
    "    \"ATP is the energy currency of the cell.\"\n",
    "]\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "doc_embeddings = model.encode(doc_chunks, show_progress_bar=True)\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "index.add(np.array(doc_embeddings).astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1b137",
   "metadata": {},
   "source": [
    "## Step 2: Retrieve Relevant Chunks Based on Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a user query\n",
    "query = \"What produces energy in a cell?\"\n",
    "query_vec = model.encode([query])\n",
    "\n",
    "D, I = index.search(np.array(query_vec).astype(\"float32\"), k=2)\n",
    "retrieved_chunks = [doc_chunks[i] for i in I[0]]\n",
    "\n",
    "print(\"Top chunks:\")\n",
    "for chunk in retrieved_chunks:\n",
    "    print(\"-\", chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a960d",
   "metadata": {},
   "source": [
    "## Step 3: Use Retrieved Context for Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee772077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(\n",
    "  api_key=api_key\n",
    ")\n",
    "\n",
    "# Construct prompt using ChatCompletion format\n",
    "context = \"\\n\".join(retrieved_chunks)\n",
    "# prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "prompt = f\"Question: {query}\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # or \"gpt-4\" if you have access\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f7837",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- This was a basic RAG pipeline.\n",
    "- Retrieved top-matching context chunks from FAISS.\n",
    "- Used the context to prompt a language model and generate a grounded answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
