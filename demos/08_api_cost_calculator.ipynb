{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b4e1469",
   "metadata": {},
   "source": [
    "# A tiny pricing engine\n",
    "\n",
    "The following module turns usage dicts into dollars. It understands:\n",
    "- Regular input tokens\n",
    "- Cached-input tokens (discounted)\\\n",
    "- Output tokens\n",
    "- Both the modern Responses API usage keys and classic prompt/completion keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03a3b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from typing import Optional, Dict, Tuple, Any, Mapping\n",
    "\n",
    "MTOK = Decimal(\"1000000\")  # per-million denominator\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelPricing:\n",
    "    input_per_mtok: Decimal\n",
    "    output_per_mtok: Decimal\n",
    "    cached_input_per_mtok: Optional[Decimal] = None  # None if not listed/applicable\n",
    "\n",
    "    def cost_from_tokens(\n",
    "        self,\n",
    "        input_tokens: int,\n",
    "        output_tokens: int,\n",
    "        cache_read_input_tokens: int = 0\n",
    "    ) -> Decimal:\n",
    "        \"\"\"\n",
    "        Compute cost:\n",
    "        - Non-cached input billed at input rate\n",
    "        - Cached-read input billed at cached_input rate (if provided)\n",
    "        - All output billed at output rate\n",
    "        \"\"\"\n",
    "        cache_read_input_tokens = max(cache_read_input_tokens, 0)\n",
    "        non_cached_input = max(input_tokens - cache_read_input_tokens, 0)\n",
    "\n",
    "        cost_input = (Decimal(non_cached_input) / MTOK) * self.input_per_mtok\n",
    "        cost_cached = Decimal(0)\n",
    "        if self.cached_input_per_mtok is not None and cache_read_input_tokens > 0:\n",
    "            cost_cached = (Decimal(cache_read_input_tokens) / MTOK) * self.cached_input_per_mtok\n",
    "        cost_output = (Decimal(output_tokens) / MTOK) * self.output_per_mtok\n",
    "\n",
    "        total = cost_input + cost_cached + cost_output\n",
    "        return total.quantize(Decimal(\"0.0000001\"), rounding=ROUND_HALF_UP)\n",
    "\n",
    "\n",
    "# Official GPT-5 prices (per 1M tokens) as of Aug 2025 — verify against pricing page.\n",
    "PRICING: Dict[str, ModelPricing] = {\n",
    "    \"gpt-5-2025-08-07\": ModelPricing(Decimal(\"1.25\"), Decimal(\"10.00\"), Decimal(\"0.125\")),\n",
    "    \"gpt-5-mini\":       ModelPricing(Decimal(\"0.25\"), Decimal(\"2.00\"),  Decimal(\"0.025\")),\n",
    "    \"gpt-5-nano\":       ModelPricing(Decimal(\"0.05\"), Decimal(\"0.40\"),  Decimal(\"0.005\")),\n",
    "    # Some apps surface a chat-aliased variant:\n",
    "    \"gpt-5-chat-latest\": ModelPricing(Decimal(\"1.25\"), Decimal(\"10.00\"), Decimal(\"0.125\")),\n",
    "}\n",
    "\n",
    "def _to_mapping(u: Any) -> Mapping[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert OpenAI SDK usage objects (e.g., ResponseUsage) or dict-like things to a plain dict.\n",
    "    Tries: model_dump() -> to_dict() -> __dict__ -> already-mapping -> attribute scan.\n",
    "    \"\"\"\n",
    "    if isinstance(u, Mapping):\n",
    "        return u\n",
    "    if hasattr(u, \"model_dump\") and callable(getattr(u, \"model_dump\")):\n",
    "        return u.model_dump()\n",
    "    if hasattr(u, \"to_dict\") and callable(getattr(u, \"to_dict\")):\n",
    "        return u.to_dict()\n",
    "    if hasattr(u, \"__dict__\"):\n",
    "        return dict(u.__dict__)\n",
    "    keys = [k for k in dir(u) if not k.startswith(\"_\")]\n",
    "    return {k: getattr(u, k) for k in keys if hasattr(u, k)}\n",
    "\n",
    "def parse_usage(u: Any) -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Make this tolerant to both Responses API and older Chat Completions keys,\n",
    "    and to different field names for prompt caching.\n",
    "\n",
    "    Returns (input_tokens, output_tokens, cache_read_input_tokens)\n",
    "    \"\"\"\n",
    "    m = _to_mapping(u)\n",
    "\n",
    "    # Modern Responses API names:\n",
    "    input_tokens = m.get(\"input_tokens\")\n",
    "    output_tokens = m.get(\"output_tokens\")\n",
    "\n",
    "    # Back-compat fallbacks (older docs/help center often say \"prompt\"/\"completion\"):\n",
    "    if input_tokens is None:\n",
    "        input_tokens = m.get(\"prompt_tokens\", 0)\n",
    "    if output_tokens is None:\n",
    "        output_tokens = m.get(\"completion_tokens\", 0)\n",
    "\n",
    "    # Prompt caching read tokens show up under several keys depending on endpoint/version:\n",
    "    cache_read = (\n",
    "        m.get(\"cache_read_input_tokens\")\n",
    "        or m.get(\"prompt_cache_read_input_tokens\")\n",
    "        or m.get(\"cached_input_tokens\")\n",
    "        or 0\n",
    "    )\n",
    "    return int(input_tokens or 0), int(output_tokens or 0), int(cache_read or 0)\n",
    "\n",
    "def cost_for_response(model: str, usage: Any) -> Decimal:\n",
    "    if model not in PRICING:\n",
    "        raise KeyError(f\"Unknown model '{model}'. Add it to PRICING.\")\n",
    "    ipt, opt, cache_read = parse_usage(usage)\n",
    "    return PRICING[model].cost_from_tokens(ipt, opt, cache_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be1c411",
   "metadata": {},
   "source": [
    "## Example: compute cost from a live API call\n",
    "\n",
    "Below is a minimal **Responses API** example using the official Python SDK. We pull `response.usage` and feed it to the pricing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923c6683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated cost for this call: $0.003864\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from decimal import Decimal\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=api_key\n",
    ")\n",
    "\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-5\",  # or gpt-5-mini / gpt-5-nano\n",
    "    input=\"Explain the difference between optimistic and pessimistic concurrency control in databases.\",\n",
    "    # Optional cost-control knobs:\n",
    "    max_output_tokens=400,   # cap outputs to bound costs\n",
    "    # verbosity=\"terse\",      # GPT-5 supports verbosity (see model docs)\n",
    "    # reasoning_effort=\"low\", # also supported across GPT-5 family\n",
    ")\n",
    "\n",
    "print(resp.output_text)  # your answer\n",
    "usd = cost_for_response(resp.model, resp.usage)\n",
    "print(f\"Estimated cost for this call: ${usd:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b128589e",
   "metadata": {},
   "source": [
    "## Token details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "772578b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_tokens': 19, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens': 384, 'output_tokens_details': {'reasoning_tokens': 384}, 'total_tokens': 403}\n"
     ]
    }
   ],
   "source": [
    "u = resp.usage.model_dump()  # or to_dict()\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f98e7b",
   "metadata": {},
   "source": [
    "## Worked cost math (so you can sanity-check numbers)\n",
    "Suppose the response usage is (replace with your numbers from the output above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"input_tokens\": 19,\n",
    "  \"output_tokens\": 384,\n",
    "  \"prompt_cache_read_input_tokens\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f72d2",
   "metadata": {},
   "source": [
    "## For gpt-5:\n",
    "\n",
    "`Non-cached input` = 19 ÷ 1,000,000 × $1.25 = $0.00002375\n",
    "\n",
    "`Cached-input` = 0 ÷ 1,000,000 × $0.125 = $0.00000000\n",
    "\n",
    "`Output` = 384 ÷ 1,000,000 × $10.00 = $0.00384000\n",
    "\n",
    "`Total` = $0.00002375 + $0.00000000 + $0.00384000 = **$0.00386375 (~0.386¢)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d585b80",
   "metadata": {},
   "source": [
    "## Pre-estimate cost before you call the API (tiktoken)\n",
    "\n",
    "If you need a budget check ahead of time, estimate tokens locally. The recommended approach is to use OpenAI’s tiktoken tokenizer (or the browser-based Tokenizer tool for quick checks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b622652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost: 0.0030188\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "import tiktoken  # pip install tiktoken\n",
    "\n",
    "def count_tokens(text: str, model: str = \"gpt-5-2025-08-07\") -> int:\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(model)\n",
    "    except Exception:\n",
    "        # Fallback – pick a close encoding if model mapping isn’t present locally\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def estimate_cost(model: str, prompt: str, expected_output_tokens: int) -> Decimal:\n",
    "    pricing = PRICING[model]\n",
    "    input_tokens = count_tokens(prompt, model)\n",
    "    # No caching assumed in a first-order estimate (set cache hits explicitly if you use it)\n",
    "    input_cost  = (Decimal(input_tokens) / MTOK) * pricing.input_per_mtok\n",
    "    output_cost = (Decimal(expected_output_tokens) / MTOK) * pricing.output_per_mtok\n",
    "    return (input_cost + output_cost).quantize(Decimal(\"0.0000001\"))\n",
    "\n",
    "# Example\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Summarize this product spec into a 5-bullet executive summary...\"\n",
    "    print(\"Estimated cost:\", estimate_cost(\"gpt-5-2025-08-07\", prompt, expected_output_tokens=300))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
